{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c057f8-3fdd-48eb-90b9-2f7be51d09c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmi\\Anaconda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as pt\n",
    "import pandas as pd\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc7cf2b-5a30-4106-a1ff-582c2d25f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonlToDataset(jsonl_path):\n",
    "    dataset = []\n",
    "    with open(jsonl_path, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    for json_str in json_list:\n",
    "        dataset.append(json.loads(json_str))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34012a5-9c5d-4912-abff-368415eee93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"D:/Deep_learning_laboratory/assignment_2/assignment_2_data/train.jsonl\"\n",
    "test_data_path = \"D:/Deep_learning_laboratory/assignment_2/assignment_2_data/test.jsonl\"\n",
    "train_dataset = jsonlToDataset(train_data_path)\n",
    "test_dataset = jsonlToDataset(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf249aa-a039-444d-a13e-9e950585f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_dataset) == 40398\n",
    "assert len(test_dataset) == 1267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b1762-3ada-4385-a1d2-01b8a00e7d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qID': '3QHITW7OYO7Q6B6ISU2UMJB84ZLAQE-2',\n",
       " 'sentence': \"Ian volunteered to eat Dennis's menudo after already having a bowl because _ despised eating intestine.\",\n",
       " 'option1': 'Ian',\n",
       " 'option2': 'Dennis',\n",
       " 'answer': '2'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc2eab-42c1-4a37-b4e0-a5fd9307f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea: use RNN (plain). we need data = (sentence, option1, option2), label = (answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21cc189-4e4e-4cd5-8d5f-e9f65173abef",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "We are going to use the library \"gensim\" for a pretrained Word2Vec model to use to get the initial embeddings of the words.\\\n",
    "We chose \"word2vec-google-news-300\", which is missing many words apparently (it doesn't have an embedding neither for the word \"a\", nor for the words \"to\", but it has an embedding for \"Ian\"...)\\\n",
    "Words which can't be embedded with word2vec will be embedded with a vector containing random values.\n",
    "\n",
    "All embedded words are vectors with 300 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cf9d1-b6b7-4386-9fa8-ed4ac69e315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load(\"D:/Deep_learning_laboratory/assignment_2/word2vec-google-news-300\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7189a9-ccd2-4120-86ff-075cbff48296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO DOWNLOAD WORD2VEC MODEL. Once downloaded, you can load weights from the folder you saved them into\n",
    "#import gensim.downloader as api\n",
    "#wv = api.load('word2vec-google-news-300')\n",
    "#wv.save(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ad8836-1412-4564-838f-480e5e358979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'capitalism'\t'communism'\t0.60\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   \n",
    "    ('car', 'bicycle'),   \n",
    "    ('car', 'airplane'),  \n",
    "    ('car', 'cereal'),    \n",
    "    ('capitalism', 'communism'), # 60% WTF\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682620b9-df06-41c4-b395-7cef49678ba4",
   "metadata": {},
   "source": [
    "### Create a sample Dataset and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb0d768-b9d3-4591-972d-3888267b7b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = []\n",
    "for i in range(80):\n",
    "    sample_dataset.append(train_dataset[i]['sentence'].split(maxsplit=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c469d90-32b2-4ed6-bb0a-05c92803d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(dataset):\n",
    "    punctuation_to_remove = string.punctuation\n",
    "    punctuation_to_remove = punctuation_to_remove.replace(\"_\",\"\")\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i] = dataset[i].translate(str.maketrans('', '', punctuation_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef780f8-8ecc-4a0f-9450-5e4d78234dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ian',\n",
       " 'volunteered',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'Denniss',\n",
       " 'menudo',\n",
       " 'after',\n",
       " 'already',\n",
       " 'having',\n",
       " 'a',\n",
       " 'bowl',\n",
       " 'because',\n",
       " '_',\n",
       " 'despised',\n",
       " 'eating',\n",
       " 'intestine']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punctuation(sample_dataset[0])\n",
    "sample_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b319e0f-1205-4e96-b0d6-88585e63b93d",
   "metadata": {},
   "source": [
    "### Stemming or Not?\n",
    "Do we need to stem words before feeding them to the Word2Vec model? Better not, the result below shows us that stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb693a-8e78-4356-8026-117a7afdae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "sample_dataset_stemmed = []\n",
    "def preprocess_words(words):\n",
    "    processed_words = []\n",
    "    stemmer=PorterStemmer()\n",
    "    for word in words:\n",
    "        processed_words.append(stemmer.stem(word))\n",
    "    return processed_words\n",
    "for i in range(80):\n",
    "    sample_dataset_stemmed.append(preprocess_words(sample_dataset[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5b427-d4c0-4beb-aadc-51a0f826739b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 278 missing words, and 370 missing stemmed words in the Word2Vec model\n"
     ]
    }
   ],
   "source": [
    "def number_of_words_of_phrase_not_in_Word2Vec(phrase):\n",
    "    numberOfWordsNotInWord2Vec = 0\n",
    "    result = []\n",
    "    for s in phrase:\n",
    "        try:\n",
    "            result.append(wv[s])\n",
    "        except KeyError:\n",
    "            numberOfWordsNotInWord2Vec = numberOfWordsNotInWord2Vec + 1\n",
    "    return numberOfWordsNotInWord2Vec\n",
    "\n",
    "numberOfWordsNotInWord2Vec = 0\n",
    "numberOfStemmedWordsNotInWord2Vec = 0\n",
    "\n",
    "for i in range(80):\n",
    "    numberOfWordsNotInWord2Vec += number_of_words_of_phrase_not_in_Word2Vec(sample_dataset[i])\n",
    "    numberOfStemmedWordsNotInWord2Vec += number_of_words_of_phrase_not_in_Word2Vec(sample_dataset_stemmed[i]) \n",
    "\n",
    "print(\"There are \" + str(numberOfWordsNotInWord2Vec) + \" missing words, and \" + str(numberOfStemmedWordsNotInWord2Vec) + \" missing stemmed words in the Word2Vec model\")\n",
    "\n",
    "#TODO\n",
    "# word \"to\" is missing. What to do with missing words?\n",
    "# Dennis is reconognized, while Dennis's is not. We need to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476a328-01b7-4055-bac4-dffaf4fa7ea2",
   "metadata": {},
   "source": [
    "### Since there are more stemmed words missing in Word2Vec with respect to unstemmed words, stemming won't be used\n",
    "## How to handle missing words? (Words that are not present in Word2Vec model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88968464-75e5-4ddb-a60f-d7585f34e2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kevin',\n",
       " 'is',\n",
       " 'far',\n",
       " 'more',\n",
       " 'physically',\n",
       " 'versatile',\n",
       " 'than',\n",
       " 'Nick,',\n",
       " 'because',\n",
       " '_',\n",
       " 'is',\n",
       " 'a',\n",
       " 'seasoned',\n",
       " 'athlete',\n",
       " 'in',\n",
       " 'training.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_phrase = sample_dataset[i]\n",
    "sample_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d5bed-1422-4ac5-b1db-c1d0b88ac236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = wv['Ian',].shape\n",
    "a = pt.FloatTensor(shape[0], shape[1])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca5189-4b2f-423d-8463-fc389ee1c0e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grand\\AppData\\Local\\Temp\\ipykernel_9712\\2224732860.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:178.)\n",
      "  sample_phrase_embeddings.append(pt.from_numpy(wv[word]))\n"
     ]
    }
   ],
   "source": [
    "sample_phrase_embeddings = []\n",
    "for word in sample_phrase:\n",
    "    try:\n",
    "        sample_phrase_embeddings.append(pt.from_numpy(wv[word]))\n",
    "    except KeyError:\n",
    "        sample_phrase_embeddings.append(pt.FloatTensor(shape[0], shape[1])) #random distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b63a9-f6d2-4754-95bc-c206aa752b5f",
   "metadata": {},
   "source": [
    "### Given a sample data turn into a format fit for a classification task\n",
    "Example:\n",
    "\n",
    "(\"a man _ a beard\", opt1=\"has\", opt2=\"have\", label=0), --- label=i, means use option i\n",
    "\n",
    "turns into\n",
    "\n",
    "(\"a man has a beard\", label=1), (\"a man have a beard\", label=0), --- label=0 means the phrase is wrong, label=1 means the phrase is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e380707-3f4d-4d12-89b3-77f323063f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qID': '3QHITW7OYO7Q6B6ISU2UMJB84ZLAQE-2',\n",
       " 'sentence': \"Ian volunteered to eat Dennis's menudo after already having a bowl because _ despised eating intestine.\",\n",
       " 'option1': 'Ian',\n",
       " 'option2': 'Dennis',\n",
       " 'answer': '2'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e204d8-15c3-4c1f-98a6-02011c214385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ian volunteered to eat Dennis's menudo after already having a bowl because Dennis despised eating intestine.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def writeOptionInPhrase(phrase, option):\n",
    "    return phrase.replace(\"_\", option)\n",
    "writeOptionInPhrase(train_dataset[0][\"sentence\"], train_dataset[0][\"option2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e83100-5e17-418c-8056-ddf7a5a3dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    preprocessed_data = []\n",
    "    preprocessed_labels = []\n",
    "    for element in dataset:\n",
    "        preprocessed_data.append(writeOptionInPhrase(element[\"sentence\"], element[\"option1\"]))\n",
    "        preprocessed_labels.append(1 if int(element[\"answer\"]) == 1 else 0)\n",
    "        preprocessed_data.append(writeOptionInPhrase(element[\"sentence\"], element[\"option2\"]))\n",
    "        preprocessed_labels.append(1 if int(element[\"answer\"]) == 2 else 0)\n",
    "    return preprocessed_data, preprocessed_labels\n",
    "preprocessed_data, preprocessed_labels = preprocess_data(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c5cc9-5b2b-470b-aaf2-3a171ad8f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our preprocessed dataset contains double the phrases of the original\n",
    "assert len(preprocessed_data) == 2*len(train_dataset)\n",
    "#the first phrase became:\n",
    "#(\"Ian volunteered to eat Dennis's menudo after already having a bowl because Ian despised eating intestine.\", 0)\n",
    "assert preprocessed_labels[0] == 0\n",
    "#(\"Ian volunteered to eat Dennis's menudo after already having a bowl because Dennis despised eating intestine.\", 1)\n",
    "assert preprocessed_labels[1] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c6dc6-a817-43c6-a371-8347a7b2e17c",
   "metadata": {},
   "source": [
    "### Now we can build a Generator class, to apply the previous steps to the whole dataset\n",
    "* x_data will contain all the embeddings of each phrase. Each phrase of n words is embedded as a list of n vectors with dimensionality 300\n",
    "* y_label will contain all possible labels. If the phrase is correct, then label=1, else label=0\n",
    "\n",
    "Oss: as explained above, we didn't use Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5cb9bf4-a01d-4096-8db5-25c4b3b5ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    x_data = []\n",
    "    y_label = []\n",
    "    embedding_shape = ()\n",
    "    word2Vec = []\n",
    "    \n",
    "    \n",
    "    def __init__(self, path_to_data):\n",
    "        dataset = jsonlToDataset(path_to_data)\n",
    "        self.remove_punctuation(dataset)\n",
    "        preprocessed_data, self.y_label = self.preprocess_data(dataset)\n",
    "        self.remove_list(dataset)\n",
    "        self.initialize_word2vec()\n",
    "        self.embed_dataset(preprocessed_data)\n",
    "        self.remove_list(preprocessed_data)\n",
    "        \n",
    "    def jsonlToDataset(self, jsonl_path):\n",
    "        dataset = []\n",
    "        with open(jsonl_path, 'r') as json_file:\n",
    "            json_list = list(json_file)\n",
    "        for json_str in json_list:\n",
    "            dataset.append(json.loads(json_str))\n",
    "        return dataset\n",
    "    \n",
    "    def remove_punctuation(self, dataset):\n",
    "        for i in range(len(dataset)):\n",
    "            dataset[i][\"sentence\"] = self.remove_punctuation_helper(\n",
    "                dataset[i][\"sentence\"])\n",
    "            \n",
    "    def remove_punctuation_helper(self, phrase):\n",
    "        punctuation_to_remove = string.punctuation\n",
    "        punctuation_to_remove = punctuation_to_remove.replace(\"_\",\"\")\n",
    "        return phrase.translate(str.maketrans('', '', punctuation_to_remove))\n",
    "    \n",
    "    def preprocess_data(self, dataset):\n",
    "        preprocessed_data = []\n",
    "        preprocessed_labels = []\n",
    "        for element in dataset:\n",
    "            preprocessed_data.append(writeOptionInPhrase(element[\"sentence\"], element[\"option1\"]))\n",
    "            preprocessed_labels.append(1 if int(element[\"answer\"]) == 1 else 0)\n",
    "            preprocessed_data.append(writeOptionInPhrase(element[\"sentence\"], element[\"option2\"]))\n",
    "            preprocessed_labels.append(1 if int(element[\"answer\"]) == 2 else 0)\n",
    "        return preprocessed_data, preprocessed_labels\n",
    "    \n",
    "    def initialize_word2vec(self):\n",
    "        from gensim.models import KeyedVectors\n",
    "        self.word2Vec = KeyedVectors.load(\"D:/Deep_learning_laboratory/assignment_2/word2vec-google-news-300\", mmap='r')\n",
    "        self.embedding_shape = self.word2Vec['Ian',].shape\n",
    "    \n",
    "    def embed_dataset(self, preprocessed_data):\n",
    "        embedded_data = []\n",
    "        for phrase in preprocessed_data:\n",
    "            self.x_data.append(\n",
    "                self.phrase_to_tensor(\n",
    "                    self.phrase_embedding(\n",
    "                        phrase.split(maxsplit=-1))))\n",
    "            \n",
    "    def phrase_embedding(self, phrase):\n",
    "        sample_phrase_embeddings = []\n",
    "        for word in phrase:\n",
    "            try:\n",
    "                sample_phrase_embeddings.append(pt.squeeze(pt.from_numpy(self.word2Vec[word])))\n",
    "            except KeyError:\n",
    "                sample_phrase_embeddings.append(pt.squeeze(pt.FloatTensor(self.embedding_shape[0], self.embedding_shape[1]))) #random distribution\n",
    "        return sample_phrase_embeddings\n",
    "    \n",
    "    def phrase_to_tensor(self, phrase):\n",
    "        return pt.stack(phrase, 0)\n",
    "    \n",
    "    def remove_list(self, list_obj):\n",
    "        del list_obj[:]\n",
    "        del list_obj\n",
    "            \n",
    "    \n",
    "gen = Generator(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd5bf250-bcd3-4059-9a0c-6f9b22961384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.6152e-02,  2.4658e-02, -1.9727e-01,  ...,  1.8848e-01,\n",
       "          1.1328e-01, -1.8945e-01],\n",
       "        [ 1.9141e-01,  2.6367e-01, -1.0840e-01,  ..., -8.2520e-02,\n",
       "          1.0986e-03, -1.7578e-02],\n",
       "        [ 1.7236e-42,  0.0000e+00,         nan,  ...,  7.2084e-31,\n",
       "          3.4649e+25, -1.8105e+28],\n",
       "        ...,\n",
       "        [ 5.2002e-02,  1.6113e-01,  2.0312e-01,  ..., -1.5039e-01,\n",
       "          2.3047e-01,  2.3145e-01],\n",
       "        [-1.4648e-01,  1.7285e-01, -4.7119e-02,  ..., -1.3245e-02,\n",
       "          7.6172e-02,  2.7344e-02],\n",
       "        [-2.5000e-01,  5.1953e-01, -1.3477e-01,  ...,  9.3994e-03,\n",
       "          3.3203e-01,  1.1963e-01]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.x_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a598b-2025-44c9-b506-45b531956e6e",
   "metadata": {},
   "source": [
    "## Model: LSTM\n",
    "The network will be a LSTM (RNN). Given a phrase our model will output whether the phrase is good or not. Half of our training set contains correct phrases, and the other half wrong ones (by construction), so the dataset is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "811feac7-3989-4bf7-833f-5aa7b5181a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_classification(pt.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_features=300, hidden_features=128, num_layers=1, out_features=1):\n",
    "        super(LSTM_classification, self).__init__()\n",
    "        \n",
    "        self.input_features=input_features\n",
    "        self.hidden_features=hidden_features\n",
    "        self.num_layers=num_layers\n",
    "        self.out_features=out_features\n",
    "        \n",
    "        self.lstm = pt.nn.LSTM(input_size = self.input_features, \n",
    "                             hidden_size = self.hidden_features, \n",
    "                             num_layers = self.num_layers)\n",
    "        self.linear = pt.nn.Linear(in_features = self.hidden_features , out_features = self.out_features)\n",
    "        \n",
    "    def forward(self, x_data):\n",
    "        \n",
    "        lstm_result = self.lstm(x_data)\n",
    "        last_layer_last_result = lstm_result[-1]\n",
    "        y_pred_tensor = self.linear(last_layer_last_result)\n",
    "        y_pred = pt.sigmoid(y_pred)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc13b6d-1ac2-47d6-a16f-3baecba866a5",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c785d1c-dfe0-4741-b36b-818dedf69a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, train_labels, test_data=None, \n",
    "                test_labels=None):\n",
    "  \n",
    "    loss_fn = pt.nn.BCELoss()\n",
    "  \n",
    "    optimiser = pt.optim.Adam(model.parameters(), lr=1e-3)\n",
    "  \n",
    "    num_epochs = 10\n",
    "  \n",
    "    train_loss_hist = np.zeros(num_epochs)  # training loss history\n",
    "    test_loss_hist = np.zeros(num_epochs)  # testing loss history\n",
    "  \n",
    "    for t in range(num_epochs):\n",
    "\n",
    "        # reset the gradients\n",
    "        #model.reset_hidden_state()\n",
    "        \n",
    "        for i in range(0, len(train_data)):\n",
    "            y_pred = model(train_data[i])\n",
    "\n",
    "            loss = loss_fn(y_pred, train_labels[i])\n",
    "\n",
    "            # zeroing the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # optimization step\n",
    "            optimizer.step()\n",
    "\n",
    "        if test_data is not None:\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_test_pred = model(test_data)\n",
    "                test_loss = loss_fn(y_test_pred, test_labels)\n",
    "            \n",
    "            test_loss_hist[t] = test_loss.item()\n",
    "\n",
    "            if t % 10 == 0 or t==99:\n",
    "                print(f\"Epoch {t} train loss: {loss.item()}, test loss: {test_loss.item()}\")\n",
    "\n",
    "            elif t % 10 == 0 or t==99:\n",
    "                print(f'Epoch {t} train loss: {loss.item()}')\n",
    "\n",
    "        train_loss_hist[t] = loss.item()\n",
    "\n",
    "    \n",
    "\n",
    "    return model.eval(), train_loss_hist, test_loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "955edc24-686f-469f-8509-01f1fd914436",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTM_classification()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_label\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [47], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_data, train_labels, test_data, test_labels)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# reset the gradients\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m#model.reset_hidden_state()\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_data)):\n\u001b[1;32m---> 19\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, train_labels[i])\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# zeroing the gradients\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [46], line 20\u001b[0m, in \u001b[0;36mLSTM_classification.forward\u001b[1;34m(self, x_data)\u001b[0m\n\u001b[0;32m     18\u001b[0m lstm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x_data)\n\u001b[0;32m     19\u001b[0m last_layer_last_result \u001b[38;5;241m=\u001b[39m lstm_result[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 20\u001b[0m y_pred_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_layer_last_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39msigmoid(y_pred)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "model = LSTM_classification()\n",
    "train_model(model, gen.x_data, gen.y_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
