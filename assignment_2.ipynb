{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c057f8-3fdd-48eb-90b9-2f7be51d09c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmi\\Anaconda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as pt\n",
    "import pandas as pd\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bc7cf2b-5a30-4106-a1ff-582c2d25f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonlToDataset(jsonl_path):\n",
    "    dataset = []\n",
    "    with open(jsonl_path, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    for json_str in json_list:\n",
    "        dataset.append(json.loads(json_str))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34012a5-9c5d-4912-abff-368415eee93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"D:/Deep_learning_laboratory/assignment_2/assignment_2_data/train.jsonl\"\n",
    "test_data_path = \"D:/Deep_learning_laboratory/assignment_2/assignment_2_data/test.jsonl\"\n",
    "train_dataset = jsonlToDataset(train_data_path)\n",
    "test_dataset = jsonlToDataset(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edf249aa-a039-444d-a13e-9e950585f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_dataset) == 40398\n",
    "assert len(test_dataset) == 1267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759b1762-3ada-4385-a1d2-01b8a00e7d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qID': '3QHITW7OYO7Q6B6ISU2UMJB84ZLAQE-2',\n",
       " 'sentence': \"Ian volunteered to eat Dennis's menudo after already having a bowl because _ despised eating intestine.\",\n",
       " 'option1': 'Ian',\n",
       " 'option2': 'Dennis',\n",
       " 'answer': '2'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdfc2eab-42c1-4a37-b4e0-a5fd9307f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea: use RNN (plain). we need data = (sentence, option1, option2), label = (answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cf9d1-b6b7-4386-9fa8-ed4ac69e315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load(\"D:/Deep_learning_laboratory/assignment_2/word2vec-google-news-300\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c7189a9-ccd2-4120-86ff-075cbff48296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO DOWNLOAD WORD2VEC MODEL. Once downloaded, you can load weights from the folder you saved them into\n",
    "#import gensim.downloader as api\n",
    "#wv = api.load('word2vec-google-news-300')\n",
    "#wv.save(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62ad8836-1412-4564-838f-480e5e358979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'capitalism'\t'communism'\t0.60\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   \n",
    "    ('car', 'bicycle'),   \n",
    "    ('car', 'airplane'),  \n",
    "    ('car', 'cereal'),    \n",
    "    ('capitalism', 'communism'), # 60% WTF\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682620b9-df06-41c4-b395-7cef49678ba4",
   "metadata": {},
   "source": [
    "### Create a sample Dataset and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fb0d768-b9d3-4591-972d-3888267b7b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = []\n",
    "for i in range(80):\n",
    "    sample_dataset.append(train_dataset[i]['sentence'].split(maxsplit=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c469d90-32b2-4ed6-bb0a-05c92803d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(dataset):\n",
    "    punctuation_to_remove = string.punctuation\n",
    "    punctuation_to_remove = punctuation_to_remove.replace(\"_\",\"\")\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i] = dataset[i].translate(str.maketrans('', '', punctuation_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fef780f8-8ecc-4a0f-9450-5e4d78234dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ian volunteered to eat Dennis's menudo after already having a bowl because _ despised eating intestine.\n",
      "Ian volunteered to eat Dennis's menudo after already having a bowl because _ despised eating intestine.\n"
     ]
    }
   ],
   "source": [
    "remove_punctuation(sample_dataset[0])\n",
    "sample_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b319e0f-1205-4e96-b0d6-88585e63b93d",
   "metadata": {},
   "source": [
    "### Stemming or Not?\n",
    "Do we need to stem words before feeding them to the Word2Vec model? Better not, the result below shows us that stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d1eb693a-8e78-4356-8026-117a7afdae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "sample_dataset_stemmed = []\n",
    "def preprocess_words(words):\n",
    "    processed_words = []\n",
    "    stemmer=PorterStemmer()\n",
    "    for word in words:\n",
    "        processed_words.append(stemmer.stem(word))\n",
    "    return processed_words\n",
    "for i in range(80):\n",
    "    sample_dataset_stemmed.append(preprocess_words(sample_dataset[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "71b5b427-d4c0-4beb-aadc-51a0f826739b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 320 missing words, and 320 missing stemmed words in the Word2Vec model\n"
     ]
    }
   ],
   "source": [
    "def number_of_words_of_phrase_not_in_Word2Vec(phrase):\n",
    "    numberOfWordsNotInWord2Vec = 0\n",
    "    for s in stri:\n",
    "        try:\n",
    "            result.append(wv[s])\n",
    "        except KeyError:\n",
    "            numberOfWordsNotInWord2Vec = numberOfWordsNotInWord2Vec + 1\n",
    "    return numberOfWordsNotInWord2Vec\n",
    "\n",
    "numberOfWordsNotInWord2Vec = 0\n",
    "numberOfStemmedWordsNotInWord2Vec = 0\n",
    "\n",
    "for i in range(80):\n",
    "    numberOfWordsNotInWord2Vec += number_of_words_of_phrase_not_in_Word2Vec(sample_dataset[i])\n",
    "    numberOfStemmedWordsNotInWord2Vec += number_of_words_of_phrase_not_in_Word2Vec(sample_dataset_stemmed[i]) \n",
    "\n",
    "print(\"There are \" + str(numberOfWordsNotInWord2Vec) + \" missing words, and \" + str(numberOfStemmedWordsNotInWord2Vec) + \" missing stemmed words in the Word2Vec model\")\n",
    "\n",
    "#TODO\n",
    "# word \"to\" is missing. What to do with missing words?\n",
    "# Dennis is reconognized, while Dennis's is not. We need to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476a328-01b7-4055-bac4-dffaf4fa7ea2",
   "metadata": {},
   "source": [
    "### Since there are the same number of stemmed words missing in Word2Vec as the number of unstemmed words, stemming won't be used\n",
    "## How to handle missing words? (Words that are not present in Word2Vec model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88968464-75e5-4ddb-a60f-d7585f34e2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'K'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_phrase = sample_dataset[i]\n",
    "sample_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3c2d5bed-1422-4ac5-b1db-c1d0b88ac236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = wv['Ian',].shape\n",
    "a = pt.FloatTensor(shape[0], shape[1])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9bca5189-4b2f-423d-8463-fc389ee1c0e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_phrase_embeddings = []\n",
    "for word in sample_phrase:\n",
    "    try:\n",
    "        sample_phrase_embeddings.append(pt.from_numpy(wv[word]))\n",
    "    except KeyError:\n",
    "        sample_phrase_embeddings.append(pt.FloatTensor(shape[0], shape[1])) #random distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b63a9-f6d2-4754-95bc-c206aa752b5f",
   "metadata": {},
   "source": [
    "### Given a sample data turn into a format fit for a classification task\n",
    "Example:\n",
    "\n",
    "(\"a man _ a beard\", opt1=\"has\", opt2=\"have\", label=0), --- label=i, means use option i\n",
    "\n",
    "turns into\n",
    "\n",
    "(\"a man has a beard\", label=1), (\"a man have a beard\", label=0), --- label=0 means the phrase is wrong, label=1 means the phrase is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e380707-3f4d-4d12-89b3-77f323063f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qID': '3QHITW7OYO7Q6B6ISU2UMJB84ZLAQE-2',\n",
       " 'sentence': \"Ian volunteered to eat Dennis's menudo after already having a bowl because _ despised eating intestine.\",\n",
       " 'option1': 'Ian',\n",
       " 'option2': 'Dennis',\n",
       " 'answer': '2'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71e204d8-15c3-4c1f-98a6-02011c214385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ian volunteered to eat Dennis's menudo after already having a bowl because Dennis despised eating intestine.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def writeOptionInPhrase(phrase, option):\n",
    "    return phrase.replace(\"_\", option)\n",
    "writeOptionInPhrase(train_dataset[0][\"sentence\"], train_dataset[0][\"option2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86e83100-5e17-418c-8056-ddf7a5a3dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    preprocessed_data = []\n",
    "    preprocessed_labels = []\n",
    "    for element in dataset:\n",
    "        preprocessed_data.append(writeOptionInPhrase(element[\"sentence\"], element[\"option1\"]))\n",
    "        preprocessed_labels.append(1 if int(element[\"answer\"]) == 1 else 0)\n",
    "        preprocessed_data.append(writeOptionInPhrase(element[\"sentence\"], element[\"option2\"]))\n",
    "        preprocessed_labels.append(1 if int(element[\"answer\"]) == 2 else 0)\n",
    "    return preprocessed_data, preprocessed_labels\n",
    "preprocessed_data, preprocessed_labels = preprocess_data(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c28c5cc9-5b2b-470b-aaf2-3a171ad8f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our preprocessed dataset contains double the phrases of the original\n",
    "assert len(preprocessed_data) == 2*len(train_dataset)\n",
    "#the first phrase became:\n",
    "#(\"Ian volunteered to eat Dennis's menudo after already having a bowl because Ian despised eating intestine.\", 0)\n",
    "assert preprocessed_labels[0] == 0\n",
    "#(\"Ian volunteered to eat Dennis's menudo after already having a bowl because Dennis despised eating intestine.\", 1)\n",
    "assert preprocessed_labels[1] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c6dc6-a817-43c6-a371-8347a7b2e17c",
   "metadata": {},
   "source": [
    "### Now we can build a Generator class, to apply the previous steps to the whole dataset\n",
    "* x_data will contain all the embeddings of each phrase. Each phrase of n words is embedded as a list of n vectors with dimensionality 300\n",
    "* y_label will contain all possible labels. If the phrase is correct, then label=1, else label=0\n",
    "\n",
    "Oss: as explained above, we didn't use Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c5cb9bf4-a01d-4096-8db5-25c4b3b5ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    x_data = []\n",
    "    y_label = []\n",
    "    embedding_shape = ()\n",
    "    word2Vec = []\n",
    "    \n",
    "    \n",
    "    def __init__(self, path_to_data):\n",
    "        dataset = jsonlToDataset(path_to_data)\n",
    "        self.remove_punctuation(dataset)\n",
    "        preprocessed_data, self.y_label = self.preprocess_data(dataset)\n",
    "        self.remove_list(dataset)\n",
    "        self.initialize_word2vec()\n",
    "        self.embed_dataset(preprocessed_data)\n",
    "        self.remove_list(preprocessed_data)\n",
    "        \n",
    "    def jsonlToDataset(self, jsonl_path):\n",
    "        dataset = []\n",
    "        with open(jsonl_path, 'r') as json_file:\n",
    "            json_list = list(json_file)\n",
    "        for json_str in json_list:\n",
    "            dataset.append(json.loads(json_str))\n",
    "        return dataset\n",
    "    \n",
    "    def remove_punctuation(self, dataset):\n",
    "        for i in range(len(dataset)):\n",
    "            dataset[i][\"sentence\"] = self.remove_punctuation_helper(\n",
    "                dataset[i][\"sentence\"])\n",
    "            \n",
    "    def remove_punctuation_helper(self, phrase):\n",
    "        punctuation_to_remove = string.punctuation\n",
    "        punctuation_to_remove = punctuation_to_remove.replace(\"_\",\"\")\n",
    "        return phrase.translate(str.maketrans('', '', punctuation_to_remove))\n",
    "    \n",
    "    def preprocess_data(self, dataset):\n",
    "        preprocessed_data = []\n",
    "        preprocessed_labels = []\n",
    "        for element in dataset:\n",
    "            preprocessed_data.append(writeOptionInPhrase(element[\"sentence\"], element[\"option1\"]))\n",
    "            preprocessed_labels.append(1 if int(element[\"answer\"]) == 1 else 0)\n",
    "            preprocessed_data.append(writeOptionInPhrase(element[\"sentence\"], element[\"option2\"]))\n",
    "            preprocessed_labels.append(1 if int(element[\"answer\"]) == 2 else 0)\n",
    "        return preprocessed_data, preprocessed_labels\n",
    "    \n",
    "    def initialize_word2vec(self):\n",
    "        from gensim.models import KeyedVectors\n",
    "        self.word2Vec = KeyedVectors.load(\"D:/Deep_learning_laboratory/assignment_2/word2vec-google-news-300\", mmap='r')\n",
    "        self.embedding_shape = self.word2Vec['Ian',].shape\n",
    "    \n",
    "    def embed_dataset(self, preprocessed_data):\n",
    "        embedded_data = []\n",
    "        for phrase in preprocessed_data:\n",
    "            self.x_data.append(self.phrase_embedding(phrase.split(maxsplit=-1)))\n",
    "            \n",
    "    def phrase_embedding(self, phrase):\n",
    "        sample_phrase_embeddings = []\n",
    "        for word in phrase:\n",
    "            try:\n",
    "                sample_phrase_embeddings.append(pt.from_numpy(self.word2Vec[word]))\n",
    "            except KeyError:\n",
    "                sample_phrase_embeddings.append(pt.FloatTensor(self.embedding_shape[0], self.embedding_shape[1])) #random distribution\n",
    "        return sample_phrase_embeddings\n",
    "    \n",
    "    def remove_list(self, list_obj):\n",
    "        del list_obj[:]\n",
    "        del list_obj\n",
    "            \n",
    "    \n",
    "gen = Generator(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dd5bf250-bcd3-4059-9a0c-6f9b22961384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gen.x_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a598b-2025-44c9-b506-45b531956e6e",
   "metadata": {},
   "source": [
    "## Model: Plain RNN\n",
    "The network will be a plain RNN. Given a phrase our model will output whether the phrase is good or not. Half of our training set contains correct phrases, and the other half wrong ones (by construction), so the dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc13b6d-1ac2-47d6-a16f-3baecba866a5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36557fe4-abfe-43c5-8513-ad47dfcc7091",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863608f5-e7a0-42f0-8c72-a658ffdacc38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
