{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "24c057f8-3fdd-48eb-90b9-2f7be51d09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as pt\n",
    "import pandas as pd\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc7cf2b-5a30-4106-a1ff-582c2d25f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonlToDataset(jsonl_path):\n",
    "    dataset = []\n",
    "    with open(jsonl_path, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    for json_str in json_list:\n",
    "        dataset.append(json.loads(json_str))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c34012a5-9c5d-4912-abff-368415eee93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"D:/Deep_learning_laboratory/assignment_2/assignment_2_data/train.jsonl\"\n",
    "test_data_path = \"D:/Deep_learning_laboratory/assignment_2/assignment_2_data/test.jsonl\"\n",
    "train_dataset = jsonlToDataset(train_data_path)\n",
    "test_dataset = jsonlToDataset(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf249aa-a039-444d-a13e-9e950585f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_dataset) == 40398\n",
    "assert len(test_dataset) == 1267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "759b1762-3ada-4385-a1d2-01b8a00e7d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ian volunteered to eat Dennis's menudo after already having a bowl because _ despised eating intestine.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdfc2eab-42c1-4a37-b4e0-a5fd9307f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea: use RNN (plain). we need data = (sentence, option1, option2), label = (answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a7cf9d1-b6b7-4386-9fa8-ed4ac69e315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load(\"D:/Deep_learning_laboratory/assignment_2/word2vec-google-news-300\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c7189a9-ccd2-4120-86ff-075cbff48296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO DOWNLOAD WORD2VEC MODEL\n",
    "#import gensim.downloader as api\n",
    "#wv = api.load('word2vec-google-news-300')\n",
    "#wv.save(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62ad8836-1412-4564-838f-480e5e358979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'capitalism'\t'communism'\t0.60\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   \n",
    "    ('car', 'bicycle'),   \n",
    "    ('car', 'airplane'),  \n",
    "    ('car', 'cereal'),    \n",
    "    ('capitalism', 'communism'), # 60% WTF\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682620b9-df06-41c4-b395-7cef49678ba4",
   "metadata": {},
   "source": [
    "### Create a sample Dataset and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8fb0d768-b9d3-4591-972d-3888267b7b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = []\n",
    "for i in range(80):\n",
    "    sample_dataset.append(train_dataset[i]['sentence'].split(maxsplit=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5c469d90-32b2-4ed6-bb0a-05c92803d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(dataset):\n",
    "    punctuation_to_remove = string.punctuation\n",
    "    punctuation_to_remove = punctuation_to_remove.replace(\"_\",\"\")\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i] = dataset[i].translate(str.maketrans('', '', punctuation_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fef780f8-8ecc-4a0f-9450-5e4d78234dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ian',\n",
       " 'volunteered',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'Denniss',\n",
       " 'menudo',\n",
       " 'after',\n",
       " 'already',\n",
       " 'having',\n",
       " 'a',\n",
       " 'bowl',\n",
       " 'because',\n",
       " '_',\n",
       " 'despised',\n",
       " 'eating',\n",
       " 'intestine']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punctuation(sample_dataset[0])\n",
    "sample_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b319e0f-1205-4e96-b0d6-88585e63b93d",
   "metadata": {},
   "source": [
    "### Stemming or Not?\n",
    "Do we need to stem words before feeding them to the Word2Vec model? Better not, the result below shows us that stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d1eb693a-8e78-4356-8026-117a7afdae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "sample_dataset_stemmed = []\n",
    "def preprocess_words(words):\n",
    "    processed_words = []\n",
    "    stemmer=PorterStemmer()\n",
    "    for word in words:\n",
    "        processed_words.append(stemmer.stem(word))\n",
    "    return processed_words\n",
    "for i in range(80):\n",
    "    sample_dataset_stemmed.append(preprocess_words(sample_dataset[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "71b5b427-d4c0-4beb-aadc-51a0f826739b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 320 missing words, and 320 missing stemmed words in the Word2Vec model\n"
     ]
    }
   ],
   "source": [
    "def number_of_words_of_phrase_not_in_Word2Vec(phrase):\n",
    "    numberOfWordsNotInWord2Vec = 0\n",
    "    for s in stri:\n",
    "        try:\n",
    "            result.append(wv[s])\n",
    "        except KeyError:\n",
    "            numberOfWordsNotInWord2Vec = numberOfWordsNotInWord2Vec + 1\n",
    "    return numberOfWordsNotInWord2Vec\n",
    "\n",
    "numberOfWordsNotInWord2Vec = 0\n",
    "numberOfStemmedWordsNotInWord2Vec = 0\n",
    "\n",
    "for i in range(80):\n",
    "    numberOfWordsNotInWord2Vec += number_of_words_of_phrase_not_in_Word2Vec(sample_dataset[i])\n",
    "    numberOfStemmedWordsNotInWord2Vec += number_of_words_of_phrase_not_in_Word2Vec(sample_dataset_stemmed[i]) \n",
    "\n",
    "print(\"There are \" + str(numberOfWordsNotInWord2Vec) + \" missing words, and \" + str(numberOfStemmedWordsNotInWord2Vec) + \" missing stemmed words in the Word2Vec model\")\n",
    "\n",
    "#TODO\n",
    "# word \"to\" is missing. What to do with missing words?\n",
    "# Dennis is reconognized, while Dennis's is not. We need to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476a328-01b7-4055-bac4-dffaf4fa7ea2",
   "metadata": {},
   "source": [
    "### Since there are the same number of stemmed words missing in Word2Vec as the number of unstemmed words, stemming won't be used\n",
    "## How to handle missing words? (Words that are not present in Word2Vec model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88968464-75e5-4ddb-a60f-d7585f34e2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kevin',\n",
       " 'is',\n",
       " 'far',\n",
       " 'more',\n",
       " 'physically',\n",
       " 'versatile',\n",
       " 'than',\n",
       " 'Nick,',\n",
       " 'because',\n",
       " '_',\n",
       " 'is',\n",
       " 'a',\n",
       " 'seasoned',\n",
       " 'athlete',\n",
       " 'in',\n",
       " 'training.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_phrase = sample_dataset[i]\n",
    "sample_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3c2d5bed-1422-4ac5-b1db-c1d0b88ac236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = wv[sample_phrase[0],].shape\n",
    "a = pt.FloatTensor(shape[0], shape[1])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9bca5189-4b2f-423d-8463-fc389ee1c0e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_phrase_embeddings = []\n",
    "for word in sample_phrase:\n",
    "    try:\n",
    "        sample_phrase_embeddings.append(pt.from_numpy(wv[word]))\n",
    "    except KeyError:\n",
    "        sample_phrase_embeddings.append(pt.FloatTensor(shape[0], shape[1])) #random distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c6dc6-a817-43c6-a371-8347a7b2e17c",
   "metadata": {},
   "source": [
    "### Now we can build a Generator class, to apply the previous steps to the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb9bf4-a01d-4096-8db5-25c4b3b5ffd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
